[{"date":"2025-08-11","title":"⭐ 智谱 GLM-4.5V","text":"**摘要：** 智谱AI发布的旗舰级开源多模态模型，基于`GLM-4.5-Air`构建。它在42个公开基准中的41个上取得SOTA性能，并以其强大的原生视频理解、视觉定位（Grounding）和网页复刻能力在社区实测中获得高度评价。\n\n**模型规格与架构：**\n- **参数量**：106B总参数，12B激活参数。\n- **架构/范式**：采用`MoE`（专家混合）架构，延续`GLM-4.1V-Thinking`技术路线，文本基座为`GLM-4.5-Air-Base`。\n- **其他特性**：新增“思考模式”开关，允许用户在追求速度和深度推理之间进行灵活切换。\n\n**性能与实测表现：**\n- **基准**：官方宣称在42个公开多模态榜单中，41个达到SOTA，综合性能领先同级别开源模型。\n- **实测**：在社区测试中，于专业读数（如游标卡尺）、复杂场景推理、网页复刻、原生视频内容分析、视觉定位等高级任务上表现出色且快速。\n\n**优势：**\n- **原生视频理解**：具备真正的时空理解能力，能按时间戳分析视频画面内容，而非简单的音频转录。\n- **强大的文档与代码能力**：在OCR、表格识别（可还原为HTML）、网页复刻等任务上表现顶尖。\n- **精准的视觉定位（Grounding）**：能准确识别并框选图像中的指定元素，在特定任务中切换到Grounding模式可显著提升准确率。\n\n**劣势：**\n- **高阶空间逻辑推理**：与其他多模态模型类似，在复杂的空间变换和逻辑推理任务上存在明显短板。\n- **细粒度识别**：在区分度极低的场景（如高度相似的建筑内景）中可能出错。\n- **部署门槛**：模型体量较大（106B），对本地部署有较高硬件要求。\n\n\n```json\n## Role\n你是一位有多年经验的OCR表格识别专家。\n## Goals\n需要通过给定的图片，识别表格里的内容，并以html表格结果格式输出结果。\n## Constrains\n- 需要认识识别图片中的内容，将每个表格单元格中的内容完整的识别出来，并填入html表格结构中；\n- 图片中的表格单元格中可能存在一些占位符需要识别出来，例如\"-\"、\"—\"、\"/\"等；\n- 输出表格结构一定遵循图片中的结构，表格结构完全一致；\n- 特别注意图片中存在合并单元格的情况，结构不要出错；\n- 对于内容较多的图片，一定要输出完整的结果，不要断章取义，更不要随意编造；\n- 图片内容需要完整识别，不要遗漏，同时注意合并单元；\n- 最终输出结果需要是html格式的表格内容。\n## Initialization\n请仔细思考后，输出html表格结果。\n```\n测试双栏检验单，慢思考模型下真的很不错。\n\n**其他 - Grounding**\n**普通问法（可能得到文本回答）：**\n`\"图片中奔跑的人在第几行第几列？\"`\n`\"图里有几串烤串？\"`\n**Grounding问法（触发图片标记）：**\n`\"请**框选出**图片中奔跑的人。\"`\n`\"请帮我**圈出**图里所有的烧烤签子。\"`\n`\"**标记出**图里的郭帆导演。`","modelSize":"106B (总计 106B, 激活 12B)","modelType":"多模态、文档解析模型、代码","openSource":true,"contextWindow":"","officialDoc":"https://github.com/zai-org/GLM-V","evaluation":"曾经的国产之光，智谱好像回来了。\n赞美开源。"},{"date":"2025-08-11","title":"Baichuan Baichuan-M2-32B","text":"**摘要：** 百川智能发布的32B开源医疗增强推理模型，在权威医疗评测`HealthBench`上超越OpenAI的`gpt-oss-120b`登顶SOTA。该模型基于创新的“大型验证器系统”和“患者模拟器”，在保持通用能力的同时，大幅提升了医疗推理能力，并支持RTX 4090单卡部署，部署成本极低。\n\n**模型规格与架构：**\n- **参数量：** `32B`\n- **基础模型：** 基于 `Qwen/Qwen2.5-32B`\n- **核心架构/范式：**\n  - **大型验证器系统 (Large Verifier System)：** 结合医疗场景特点设计，包含“患者模拟器”和多维度验证机制，为强化学习提供高质量、动态的奖励信号。\n  - **医疗领域自适应增强：** 采用中期训练（Mid-Training）方式高效注入医疗知识，同时保持通用能力。\n  - **多阶段强化学习 (Multi-Stage RL)：** 采用改进的`GRPO`算法，分阶段培养模型的医学知识、推理和医患交互能力。\n- **上下文：** `32K`\n\n**性能与实测表现：**\n- **医疗基准 (`HealthBench`)：**\n  - 标准版得分 `60.1`，超越 `gpt-oss-120b`（57.6分）和 `Qwen3-235B`（55.2分）。\n  - 困难版 (`HealthBench-Hard`) 得分 `34.7`，远超 `gpt-oss-120b`（30分），与 `GPT-5` 是唯二超过32分的模型。\n- **通用能力基准：** 在`AIME24/25`、`Arena-Hard-v2.0`、`WritingBench`等通用评测上，全面超越同量级的`Qwen3-32B`。\n- **本土化实测：** 在中国临床诊疗场景（如肝癌治疗案例）中，其推荐方案比`gpt-oss`系列更贴合国内权威指南，本土化优势明显。\n\n**优势：**\n- **性能卓越：** 以更小模型尺寸在医疗评测上实现SOTA，尤其在复杂、困难场景下表现突出。\n- **成本极低：** 经过4-bit量化后支持单张消费级显卡（RTX 4090）部署，极大降低了私有化门槛。\n- **技术创新：** 首创“患者模拟器”和大型验证器系统，为解决医疗领域AI训练难题提供了新范式。\n- **能力均衡：** 在强化医疗能力的同时，通用核心能力不降反增。\n\n**适用场景 / 备注：**\n- 适用于临床辅助诊断、智能问诊、医学知识问答与教育、健康咨询等场景。\n- 官方强调，模型仅供研究和参考，不能替代专业的医疗诊断或治疗。","modelSize":"32B","modelType":"语言, 医疗","openSource":true,"contextWindow":"32K","officialDoc":"https://www.baichuan-ai.com/blog/baichuan-M2","evaluation":"32B太香了，但具体效果如何，还得在实际业务中见真章。"},{"date":"2025-08-08","title":"⭐OpenAI GPT-5","text":"OpenAI于2025年8月8日正式发布其下一代旗舰模型GPT-5。该模型被定义为一个统一的集成系统，而非单一模型。\n\n**核心架构与模型系列：**\nGPT-5系统整合并重命名了多个先前的模型，形成了一个统一的家族。其核心是“快思考”的`main`系列和“慢思考”的`thinking`系列，由一个智能路由器根据任务需求自动调用。详细的模型继承关系如下：\n* `GPT-4o` -> `gpt-5-main`\n* `GPT-4o-mini` -> `gpt-5-main-mini`\n* `OpenAI o3` -> `gpt-5-thinking`\n* `OpenAI o4-mini` -> `gpt-5-thinking-mini`\n* `GPT-4.1-nano` -> `gpt-5-thinking-nano`\n* `OpenAI o3 Pro` -> `gpt-5-thinking-pro`\n\n此对应关系明确显示，`main`系列是GPT-4o的直接延续，而`thinking`系列则整合了之前独立的、推理能力更强的o3、o4等实验性模型分支。\n\n**性能与实测表现：**\n* **基准测试**: 在数学、编程、多模态等多个基准测试中全面领先，成为大模型盲测竞技场的新榜首。同时，模型效率也得到提升，在完成复杂任务时输出的Token减少了50-80%。\n* **减少幻觉与谄媚**: 大幅降低了事实性错误的发生率（`gpt-5-thinking`比o3少五倍以上），并减少了不必要的迎合，对话体验更像与真人交流。\n* **编程能力 (高度好评)**: 被评为“一骑绝尘”，尤其在前端UI/UX设计、生产级代码的精准修改和高上下文精度方面表现惊人。实测中能从一句话生成复杂的可视化效果、可玩游戏，并完成了其他顶级模型失败的编码任务。\n* **写作能力 (评价分裂)**: 受到部分用户，特别是重度写作者的批评，认为其文笔相较于已被移除的GPT-4.5有明显退步，风格变得蹩脚，指令遵循能力一般。\n\n**定价与API：**\n* **价格**: API定价极具竞争力，输入为$1.25/百万token，输出为$10/百万token，低于主要竞争对手。\n* **API升级**: API迎来了多项对开发者友好的新特性。","modelSize":"","modelType":"多模态","openSource":false,"contextWindow":"400K","officialDoc":"https://openai.com/gpt-5/","evaluation":""},{"date":"2025-08-07","title":"MinerU2","text":"上海人工智能实验室发布了全新架构的智能文档解析引擎MinerU2，基于 **“通专融合”** 技术路线，旨在将复杂文档（特别是科学文献）精准转化为高质量的“AI-Ready”数据。\n\n### 模型规格与性能\n* **模型**: 端到端多模态文档解析大模型\n* **参数量**: **0.9B**\n* **性能**: 解析准确率较前代提升 **22%**，速度提升 **6倍**。性能比肩72B大模型，可在消费级显卡上运行。在同级别开源模型综合评分中位列第一。\n* **核心能力**: 除了高精度解析文本、布局、表格、公式外，新增了对化学分子式和化学反应的解析能力。\n\n### 主要功能更新 (客户端/网页端)\n* **化学解析 (MinerU.Chem)**: 新增化学论文解析能力，可提取化合物结构和化学反应，并支持交互式预览和导出。\n* **多模式翻译**: 上线全文和划词翻译功能，支持GPT-4o-mini、DeepL、Google等多种翻译引擎。\n* **知识管理**: 新增收藏、批注、反馈功能，提升用户交互和知识沉淀效率。\n* **工作流集成**: 支持一键导出到Notion和Dify，并可为超长文档设置按页码范围解析。\n\n### 生态与部署\n* 项目已在GitHub开源，并提供了桌面客户端、网页端、API及企业级服务等多种使用形态。","modelSize":"0.9B","modelType":"文档解析模型","openSource":true,"contextWindow":"","officialDoc":"https://github.com/opendatalab/MinerU","evaluation":"实测下来不及预期，在医疗检验单场景下测试，发现以下主要问题：\n - **版面理解能力不足**：出现将文本标题错误识别为图片的情况。\n - **表格解析准确率低**：表格内的数据提取错误率偏高。"},{"date":"2025-08-07","title":"OpenAI GPT-OSS","text":"OpenAI于2025年8月初发布了其自GPT-2以来的首个“开放权重”(open-weight)模型系列——GPT-OSS，包含gpt-oss-120b和gpt-oss-20b两个模型，采用Apache 2.0许可。这意味着模型权重公开，但训练代码和数据集不公开。\n\n**模型规格与架构:**\n- **gpt-oss-120b**: 拥有1170亿总参数，采用混合专家（MoE）架构，每个token激活51亿参数。官方称其性能接近闭源的o4-mini，可在单张80GB GPU上运行。\n- **gpt-oss-20b**: 拥有210亿总参数，激活36亿参数。性能接近o3-mini，可在16GB内存的消费级设备（笔记本、手机）上运行。\n两个模型均采用Transformer架构，支持128k上下文窗口，使用了类似GPT-3的交替稀疏注意力模式、分组多查询注意力（GQA），并开源了其o200k_harmony分词器。后训练流程（SFT、RL）与o4-mini相似，并支持高、中、低三种推理级别。\n\n**性能表现:**\n官方基准测试显示，GPT-OSS在工具调用（搜索、代码执行）、思维链推理和特定基准（如HealthBench）上表现强劲，部分指标甚至超越了GPT-4o和o1。\n然而，多方独立实测揭示了其优势与短板：\n- **优势**: 在结构化的数学和逻辑推理任务上表现非常出色，能够清晰地解决复杂的数学题。\n- **劣势**: 在更泛化的任务上表现不佳。尤其在代码生成方面，无法完成稍复杂的项目，表现“一言难尽”，远不如Qwen3等竞品；在需要整合多条信息的复杂推理题（如七个小矮人）上会出错或卡死；在创意性写作和角色扮演方面也同样逊色。","modelSize":"120B (总计 117B, 激活 5.1B), 20B (总计 21B,激活 3.6B)","modelType":"语言大模型","openSource":true,"contextWindow":"128k","officialDoc":"https://huggingface.co/collections/openai/gpt-oss-68911959590a1634ba11c7a4","evaluation":"官方数据惊艳，实测表现拉垮。擅长数学推理，但在编程和复杂通用任务上远不及预期，综合能力不如同级别的国产开源模型。"},{"date":"2025-08-05","title":"Qwen-Image","text":"阿里千问团队发布的Qwen-Image是其首个图像生成基础模型，基于20B参数的MMDiT架构。该模型在文生图方面表现出色，尤其擅长中英文文本的精准渲染，能将文字与图像无缝融合。它支持通用图像编辑（如风格迁移、物体增删、姿态调整）、多种艺术风格生成，并能处理物体检测、语义分割等计算机视觉任务。Qwen-Image在多个公开基准测试中取得SOTA，是当前Artificial Analysis Image Arena上排名最高的开源模型。\n\n**MMDiT**，全称为**多模态扩散变换器（Multimodal Diffusion Transformer）**，是当前AI图像生成领域一项前沿的核心技术架构。它是一种专门设计用来处理和融合多种不同类型数据（即“模态”），如文本描述和图像信息的神经网络模型。\n\n这项技术是继“扩散模型（Diffusion Model）”和“视觉变换器（Vision Transformer, ViT）”之后的又一重要演进，通过其独特的结构，显著提升了AI模型根据复杂文字提示生成高质量、高相关性图像的能力。\n\n**核心思想：为不同信息类型设立“专属通道”**\n在传统的图像生成模型中，文本指令通常通过一个称为“交叉注意力（Cross-Attention）”的机制被注入到图像生成过程中。你可以把它想象成，在处理图像的同时，模型会时不时地“瞥一眼”文本指令来获取引导。\n\nMMDiT则采用了更为精妙的方法。它的核心创新在于：\n\n1. **分离处理**：MMDiT为文本信息和图像信息设立了两套独立的处理模块（或使用独立的权重）。这意味着文本和图像的特征（Tokens）在各自的“专属通道”中被分别处理，保留了各自模态的独特性和完整性。\n\n2. **联合注意力**：在经过初步的独立处理后，两条通道中的信息会被**拼接（concatenate）**在一起，送入一个统一的注意力模块中进行联合处理。这使得文本和图像信息能够在更深层次上进行交互和对齐，模型可以更精确地理解“宇航员”、“骑着猪”、“穿着芭蕾舞裙”等不同概念应该如何组合在同一画面中。\n\n简单来说，MMDiT架构就像一个拥有两个独立专业团队（一个文本专家团队，一个图像专家团队）的工作室。两个团队先分别消化自己的任务信息，然后在关键节点上进行高效的协同工作，共同完成最终的创作。","modelSize":"20B (MMDiT)","modelType":"图像生成、图像编辑","openSource":true,"contextWindow":"","officialDoc":"https://huggingface.co/Qwen/Qwen-Image","evaluation":"左手“万相”拍视频，右手“千问”P图片，阿里这是要承包我的创意文件夹吗？"},{"date":"2025-08-04","title":"Hunyuan 腾讯混元小尺寸模型系列","text":"### 核心发布：端侧开源小尺寸语言模型\n腾讯于2025年8月4日宣布，正式开源四款专为**端侧设备**设计的小尺寸语言模型，旨在推动AI在消费电子、智能汽车等领域的应用。\n同时​增强了Agent能力，在BFCL-v3、τ-Bench、C3-Bench等智能体基准测试中领先\n\n### 模型规格\n该系列包含四个不同规模的模型，以满足不同场景的需求：\n* **0.5B (5亿参数)**\n* **1.8B (18亿参数)**\n* **4B (40亿参数)**\n* **7B (70亿参数)**\n\n### 主要优势\n* **高效运行**: 能够在笔记本、手机、智能座舱等低功耗设备上高效部署。\n* **易于微调**: 支持在垂直领域进行低成本的定制化微调。\n\n### 其他近期开源\n此次发布是腾讯近期密集开源动作的一部分，此前还开源了**混元3D世界模型1.0**（2025年7月）和**混元Hunyuan-A13B** MoE模型（2025年6月）。","modelSize":"0.5B, 1.8B, 4B, 7B","modelType":"语言大模型（快思考和慢思考）","openSource":true,"contextWindow":"256K","officialDoc":"https://huggingface.co/tencent-hunyuan","evaluation":"小小杯、超小杯、超超小杯"},{"date":"2025-08-01","title":"阶跃Step-3","text":"#### \n阶跃星辰最新开源的Step-3是一款SOTA级基础模型，其核心设计理念是 **“模型-Infra协同设计”** ，旨在实现极致的推理效率和性价比。它在MMMU等多个多模态榜单上取得了开源模型的新SOTA成绩。\n\n### 模型规格\n* **架构**: MoE (混合专家) 架构，包含48个专家\n* **参数量**: 总参数量 321B (316B语言 + 5B视觉)，激活参数量 38B\n* **性能**: 在Hopper GPU上解码速度高达 **4039 token/秒**，是DeepSeek-V3的174%（4K上下文、FP8、无MTP）。\n* **成本**: 性价比极高，在特定硬件组合下，**百万Token解码成本不到4毛钱人民币**，在H20计算卡上解码成本仅为DeepSeek-V3的30%。\n\n### 技术亮点\n* **模型-Infra协同设计**: 将模型架构、系统调度和硬件Infra作为一个整体进行优化。\n* **MFA注意力机制**: 采用自研的MFA（多矩阵因子分解）注意力机制，从根源上压缩KV缓存，降低计算消耗，尤其适合长上下文场景。\n* **AFD解耦系统**: 提出AFD（Attention-FFN Disaggregation）机制，将Attention和FFN计算任务拆分到各自最适合的GPU集群，通过自研的**StepMesh通信库**连接，实现整体吞吐效率最大化。\n* **开源开放**: 模型及核心的StepMesh通信库均已开源。\n\n### 部署能力\n* 可在8块48GB的GPU上运行，并处理高达80万个Token的上下文。","modelSize":"321B (激活 38B)","modelType":"多模态大模型","openSource":true,"contextWindow":"64K","officialDoc":"https://github.com/stepfun-ai/Step3"},{"date":"2025-07-30","title":"⭐dots.ocr","text":"dots.ocr是由小红书发布的、基于17亿参数大语言模型的多语言文档解析模型。它将版面检测与内容识别统一在单一的视觉语言模型中，在OmniDocBench等多个基准测试中，其文本、表格及阅读顺序解析能力均达到SOTA（业界最佳）水平，并对低资源语言有出色的支持。","modelSize":"1.7B","modelType":"文档多模态模型/文档解析","openSource":true,"contextWindow":"","officialDoc":"https://github.com/rednote-hilab/dots.ocr","evaluation":"1.7B小钢炮，实测效果好于Mineru。开源SOTA级别，官方提供的评测结果中，遥遥领先于其他多款模型或工具。"},{"date":"2025-07-30","title":"Qwen3-30B-A3B-Instruct-2507","text":"\n- 显著提升了包括 指令执行、逻辑推理、文本理解、数学、科学、编码和工具使用 在内的通用能力。\n- 大幅增加 了 多种语言 中的长尾知识覆盖范围。\n- 显著更好地符合 用户在 主观和开放式任务 中的偏好，能够提供更有帮助的响应和更高质量的文本生成。","modelSize":"30.5B(激活 3.3B)","modelType":"","openSource":true,"contextWindow":"256K","officialDoc":""},{"date":"2025-07-30","title":"Qwen3-30B-A3B-Thinking-2507","text":"- 在逻辑推理、数学、科学、编码和通常需要人类专业知识的学术基准等推理任务上显著提升的性能。\n - 明显更好的通用能力，如指令执行、工具使用、文本生成以及与人类偏好的一致性。\n - 增强的256K长上下文理解能力。\n此版本的思考长度有所增加。建议在高度复杂的推理任务中使用它。","modelSize":"30.5B(3.3B)","modelType":"","openSource":true,"contextWindow":"256K","officialDoc":""},{"date":"2025-07-28","title":"⭐(智谱AI) GLM-4.5","text":"智谱（Zhipu）发布的GLM-4.5是一款强大的多模态模型，拥有3550亿总参数和320亿激活参数。该模型在复杂推理和代码生成方面表现出色，是面向智能体应用的下一代基础模型。","modelSize":"355B(激活 32B), 106B(激活 12B)。GLM-4.5: 总参数355B，激活参数32B；GLM-4.5-Air: 总参数106B，激活参数12B","modelType":"多模态大模型","openSource":true,"contextWindow":"128k","officialDoc":"https://huggingface.co/ZhipuAI/GLM-4.5-32B-Code-Instruct"},{"date":"2025-07-28","title":"通义万相2.2 (Tongyi Wanxiang 2.2)","text":"通义万相2.2是阿里巴巴于2025年7月28日发布的、主打**视频生成**的新一代AI模型，其目标是实现“电影级”的创作水准。\n\n### 主要功能\n* **文生视频 (Text-to-Video)**：根据文本描述生成高清视频。\n* **图生视频 (Image-to-Video)**：将静态图片转化为内容和风格一致的动态视频。\n* **统一视频模型**：发布了一款5B参数的统一模型，同时支持文生视频和图生视频，降低了部署门槛。\n* **文生图**：系列中也包含了升级的文生图模型。\n\n### 技术亮点\n* **首创MoE架构**: 业界首个在视频生成中采用混合专家（MoE）架构的模型，有效平衡了效果与计算效率。\n* **电影美学控制系统**: 内置超过60个美学参数，允许用户精细控制光影、色彩、镜头语言等视觉风格。\n* **强大的动态表现**: 在处理复杂运动、保持角色一致性和还原微表情等细节方面表现出色。","modelSize":"27B (MoE, 激活14B), 5B","modelType":"视频生成","openSource":true,"contextWindow":"","officialDoc":"https://modelscope.cn/organization/iic"},{"date":"2025-07-27","title":"Qwen3-Coder-Plus","text":"阿里巴巴（Alibaba）发布的Qwen3-Coder-Plus是Qwen3系列中专为代码生成和补全而优化的模型。它在多种编程语言和框架上都表现出色，能够显著提升开发效率。","modelSize":"","modelType":"文本 (代码)","openSource":true,"contextWindow":"","officialDoc":"https://huggingface.co/Qwen/Qwen3-Coder-72B-G-Instruct"},{"date":"2025-07-26","title":"Intern-S1","text":"上海人工智能实验室（Shanghai AI Laboratory）发布的Intern-S1是一款科学多模态大模型。它能够理解和处理复杂的科学数据，包括图表、公式和实验图像，在科研领域具有广泛的应用前景。","modelSize":"","modelType":"多模态","openSource":true,"contextWindow":"","officialDoc":"https://huggingface.co/OpenGVLab/Intern-S1"},{"date":"2025-07-25","title":"阿里巴巴 Qwen3-235B-A22B-Thinking-2507","text":"Qwen3-Thinking 是阿里巴巴Qwen团队推出的 MoE 模型，专注于复杂认知任务、多步推理和抽象思维，旨在处理高级逻辑和分析任务。","modelSize":"235B(激活 22B)","modelType":"语言模型 (MoE，推理强化)","openSource":true,"contextWindow":"128K","officialDoc":"https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507"},{"date":"2025-07-25","title":"阿里巴巴 Qwen3-Coder-480B-A35B-Instruct","text":"","modelSize":"480B(激活 35B)","modelType":"代码","openSource":true,"contextWindow":"","officialDoc":""},{"date":"2025-07-25","title":"Coze / Coze Loop","text":"字节跳动（ByteDance）开源的Coze和Coze Loop是AI智能体开发平台，旨在简化和加速AI应用的开发和部署。这些平台提供了一系列工具和组件，帮助开发者快速构建和迭代AI智能体。","modelSize":"N/A (框架)","modelType":"框架","openSource":true,"contextWindow":"N/A","officialDoc":"https://www.coze.com/"},{"date":"2025-07-23","title":"Qwen3-Coder","text":"Qwen3-Coder 是通义千问最新开源的智能体代码模型，在智能体编程、智能体浏览和基础编程任务中表现出色。它具有高度智能体化特性，旨在大幅提升开发效率和代码质量，其性能可与Claude Sonnet媲美，并支持工具调用。","modelSize":"450B(激活 35B)","modelType":"代码生成 (智能体编程)","openSource":true,"contextWindow":"256K (可扩展至1M)","officialDoc":"https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct"},{"date":"2025-07-22","title":"阿里巴巴 Qwen3-Coder-480B-A35B-Instruct","text":"","modelSize":"","modelType":"代码","openSource":true,"contextWindow":"","officialDoc":""},{"date":"2025-07-21","title":"阿里巴巴 Qwen3-235B-A22B-Instruct-2507","text":"","modelSize":"","modelType":"代码","openSource":true,"contextWindow":"","officialDoc":""},{"date":"2025-07-14","title":"RoboBrain 2.0 / RoboOS 2.0","text":"智源研究院（BAAI）发布的RoboBrain 2.0和RoboOS 2.0是面向机器人的下一代操作系统和大脑。它们旨在为机器人提供更强的感知、决策和执行能力，推动机器人技术的发展。","modelSize":"N/A (框架)","modelType":"智能体框架","openSource":true,"contextWindow":"N/A","officialDoc":""},{"date":"2025-07-11","title":"KAT-V1-40B","text":"快手（Kuaishou）发布的KAT-V1-40B是一款400亿参数的文本模型，旨在解决复杂推理中的“过度思考”问题。它通过优化推理路径，提升了模型的效率和准确性。","modelSize":"40B","modelType":"语言模型","openSource":true,"contextWindow":"","officialDoc":""},{"date":"2025-07-11","title":"⭐Kimi K2","text":"月之暗面（Moonshot AI）发布的Kimi K2是一款万亿级参数的MoE模型，拥有320亿激活参数。它在长文本处理方面表现卓越，支持128k的上下文长度，并显著提升了中文能力。","modelSize":"1T(激活 320B)","modelType":"语言模型","openSource":true,"contextWindow":"128k","officialDoc":"https://huggingface.co/moonshot-ai/Kimi-K2-32B"},{"date":"2025-07-04","title":"MOSS-TTSD-v0.5","text":"复旦大学（Fudan University）发布的MOSS-TTSD-v0.5是一款基于Qwen3-1.7B-base的双语语音合成模型。它支持长达960秒的语音生成，并在音色、韵律和自然度方面表现出色。","modelSize":"1.7B","modelType":"多模态 (语音生成)","openSource":true,"contextWindow":"960秒","officialDoc":"https://huggingface.co/fnlp/MOSS-TTSD-v0.5"}]